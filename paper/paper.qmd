---
title: "Predicting the 2024 US Presidential Election with a Model-Based Forecast"
subtitle: "Using Generalized Linear Models to Predict Election Outcomes"
author: 
  - Yuanyi (Leo) Liu
  - Dezhen Chen
  - Ziyuan Shen
thanks: "Code and data are available at: [Forecasting the 2024 US Presidential Election](https://github.com/leoyliu/Forecasting-the-2024-US-Presidential-Election)."
date: today
date-format: long
abstract: "In this paper, we use a logistic regression model based on \"poll-of-polls\" data to predict the winner of the 2024 U.S. presidential election. By combining multiple polls, the model smooths out short-term fluctuations in surveys to make more reliable estimates of voter preferences. The findings show early trends in the leading candidates and reflect public opinion and its evolution through the election period. This prediction improves understanding of electoral trends and helps to explain the changing political scene more clearly"
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(palmerpenguins)
```


# Introduction {#sec-intro}

The presidential election has a significant impact on national and global policies, which makes election forecasting a valuable tool for understanding political outcomes. However, individual polls often contain biases and short-term fluctuations that can obscure long-term trends. To deal with these challenges, we use a ‘poll-of-polls’ approach that combines multiple polls to provide more stable predictions of voter preferences over time. In this paper, we use national polling data from the period leading up to the 2024 U.S. presidential election to generate a logistic model that predicts the winner while capturing changes in public opinion throughout the election period.

The model estimates the probability of a candidate winning an election based on aggregated polling data. Logistic regression is used for binary outcomes such as election results, which allows us to model the likelihood of each candidate winning as new data becomes available. By taking into account changes in opinion polls and tracking trends in public opinion over time, this model improves the accuracy of predictions.

Our analysis identifies key trends among the major candidates and tracks changes in public sentiment throughout the campaign. The results suggest that aggregated polls can reduce some uncertainty by smoothing short-term fluctuations between surveys, but uncertainty cannot be eliminated. A sudden change in public opinion can still change the path of an election campaign, which highlights the importance of continuous polling monitoring.

A reliable election forecast can influence public expectations, and campaign strategies, and increase the visibility of reporting on election trends. An accurate forecast can also increase voter participation by providing individuals with a clearer perspective of the electoral landscape, and help them participate more effectively in the process of democracy. Prediction models like ours not only help with resource allocation for campaigns but also support the wider public by providing them with a clearer understanding of political dynamics.

The structure of the paper is organized as follows: following @sec-intro, @sec-data presents the data collection and cleaning process, along with an overview of the variables used in the analysis. @sec-model introduces the forecasting models, explaining why the selected models are suitable for predicting election outcomes based on aggregated polling data. Then @sec-result presents the main findings, including detailed crime trends for each neighborhood and year. Finally, @sec-discussion provides the results, highlighting key trends and predictions. Eventuallt, @sec-discussion concludes with a discussion of the findings, evaluating the reliability of the forecasts and identifying potential limitations of the models.


# Data {#sec-data}

## Overview

We use the statistical programming language R [@citeR] to process and analyze polling data for the 2024 US Presidential election. The data, obtained from a repository of publicly available polls, includes variables from numerous pollsters across the country [@citedata]. The aim is to create a reproducible forecast model by focusing on high-quality polls and narrowing our scope to Kamala Harris as a candidate of interest. Following methodologies discussed by "Telling Stories with Data" [@tellingstories], we examine how polling data reflects electoral behavior and voter preferences.

The dataset includes 15,891 rows and 52 columns, covering various pollster attributes such as pollster name, state, methodology, and polling results. We filter the dataset to include only high-quality polls, i.e., those with a numeric grade of 3.0 or higher, ensuring that only reputable and well-documented pollster results are used in our model. Additionally, we focus solely on polls that include Kamala Harris as a candidate, which helps narrow our analysis to a specific electoral scenario.

## Measurement
	
Polling data captures voter preferences and electoral forecasts by collecting responses from a representative sample of the population. In this case, pollsters attempt to gauge public opinion by surveying individuals on their preferred candidate, adjusting for demographic factors and political trends. The raw data entries reflect the outcomes of these surveys.

Pollsters often rely on random sampling methods to recruit participants, as discussed in [@ todo], ensuring a diverse and representative group. Some pollsters use online platforms, while others use phone interviews or in-person surveys. The dataset captures these variations through variables such as `methodology` and `sample_size`. Sampling errors, response biases, and adjustments (e.g., weighting respondents based on age, race, or geographic region) are part of how this world phenomenon is translated into data entries.

The process begins with identifying the population of interest—likely voters or registered voters—and applying statistical adjustments to the raw responses. Pollsters handle non-response through weighting or imputation methods, ensuring that missing data or low response rates do not skew the results significantly. The column `numeric_grade` reflects how well these pollsters adhere to rigorous methodology, allowing us to filter for only the most reliable sources in our analysis.

## Outcome variables

### Percent Support (pct)
The primary outcome variable in this dataset is the percentage of respondents who support a given candidate. This is represented by the pct variable, which is reported by pollsters for each candidate they survey. For Kamala Harris, this percentage reflects her standing in polls, and variations in this metric can indicate shifts in voter preference over time.

Below is a summary table showing the average, minimum, and maximum percent support for Kamala Harris across high-quality polls:



Some of our data is of penguins (@fig-bills), from @palmerpenguins.

```{r}
#| label: fig-bills
#| fig-cap: Bills of penguins
#| echo: false

ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"),
                    guide = "none") +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

Talk more about it.

And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)

```{r}
#| label: fig-planes
#| fig-cap: Relationship between wing length and width
#| echo: false
#| warning: false
#| message: false

```

Talk way more about it. 

## Predictor variables

### Pollster Name (display_name)
This variable represents the pollster responsible for each entry in the dataset. Different pollsters may use varying methods and possess differing levels of reliability, which the numeric_grade helps quantify. High-quality pollsters (numeric_grade ≥ 3.0) are emphasized in this analysis to ensure the robustness of the forecast.

### Internal Polling (internal)
The variable internal is a binary indicator that identifies whether a poll was commissioned by a political campaign or interest group. Internal polls can sometimes be biased, either in how respondents are selected or how results are reported. However, the filtering for high-quality pollsters ensures that even internal polls meet certain standards of rigor.

### Partisan Polling (partisan)
Like internal polling, partisan polling can introduce bias, as it may be sponsored by organizations with a vested interest in the election outcome. The partisan variable, similar to the internal variable, flags such polls. In the dataset, these are rare but still important to identify, especially when making impartial predictions.

### Party Affiliation (party)
The party variable indicates the political party that the poll respondent supports. This is key to understanding the partisan leanings of the sample. For Kamala Harris, her affiliation with the Democratic Party is represented in this column.

Below is a table summarizing the distribution of party affiliations in polls that include Kamala Harris:

### Race ID (race_id)
This variable identifies the electoral race or contest being polled. In this analysis, the focus is on the 2024 presidential election. Tracking results across different races (i.e., primary versus general election) helps in understanding shifts in voter opinion throughout the campaign.

In summary, the dataset offers a rich variety of variables that capture voter sentiment, pollster reliability, and electoral dynamics. By carefully filtering and analyzing this data, we can build a robust model to forecast the outcome of the 2024 US Presidential election.


# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results {#sec-result}

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <-
  readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```




# Discussion {#sec-discussion}

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

```



\newpage


# References


